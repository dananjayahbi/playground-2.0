{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dynamic Sparse Activation (DSA) Gate ---\n",
    "class DSAGate(nn.Module):\n",
    "    def __init__(self, in_channels, sparsity=0.1):\n",
    "        super().__init__()\n",
    "        self.sparsity = sparsity\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d(1),  # Reduce spatial dimensions to (B, C, 1, 1)\n",
    "            nn.Flatten(),  # Flatten to shape (B, C)\n",
    "            nn.Linear(16, in_channels)  # Transform to (B, in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.gate(x).squeeze()  # (B, C)\n",
    "\n",
    "        # Compute per-batch threshold, instead of a single global one\n",
    "        thresholds = torch.quantile(scores, 1 - self.sparsity, dim=1, keepdim=True)  # (B, 1)\n",
    "\n",
    "        mask = (scores > thresholds).float()  # (B, C)\n",
    "\n",
    "        # Ensure the mask is reshaped properly to match (B, C, H, W)\n",
    "        return x * mask.view(x.shape[0], x.shape[1], 1, 1)  # Correct broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Associative Memory Module (AMM) ---\n",
    "class AMM(nn.Module):\n",
    "    def __init__(self, prototype_dim):\n",
    "        super().__init__()\n",
    "        self.prototype_dim = prototype_dim\n",
    "        self.prototypes = faiss.IndexFlatL2(prototype_dim)\n",
    "        self.prototype_data = []\n",
    "\n",
    "    def add_prototypes(self, embeddings):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy().astype('float32')\n",
    "        if len(self.prototype_data) == 0:\n",
    "            self.prototypes.add(embeddings_np)\n",
    "        else:\n",
    "            self.prototypes.add(np.concatenate([self.prototype_data[-1], embeddings_np]))\n",
    "        self.prototype_data.append(embeddings_np)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_np = x.detach().cpu().numpy().astype('float32')  # ✅ FIXED\n",
    "        _, indices = self.prototypes.search(x_np, 1)\n",
    "        return torch.stack([torch.from_numpy(self.prototype_data[0][idx[0]]) for idx in indices]).to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Network ---\n",
    "class NDSPC_CatDetector(nn.Module):\n",
    "    def __init__(self, img_size=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.dsa_gate = DSAGate(16, sparsity=0.2)\n",
    "        \n",
    "        # Calculate prototype dimension based on conv1 output\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, img_size, img_size)\n",
    "            features = self.conv1(dummy_input)\n",
    "            self.prototype_dim = features.numel() // features.shape[0]\n",
    "        \n",
    "        self.amm = AMM(self.prototype_dim)\n",
    "        self.predictor = nn.Linear(self.prototype_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dsa_gate(x)\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        prototype = self.amm(x_flat)\n",
    "        return torch.sigmoid(self.predictor(prototype)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "SPARSITY = 0.2\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 64\n",
    "torch.backends.cudnn.benchmark = True  # Optimize performance\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda:0\"\n",
    "    torch.cuda.empty_cache()  # Clears any cached memory\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"data/train\", transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = NDSPC_CatDetector(img_size=IMG_SIZE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Prototypes: 100%|██████████| 38/38 [00:00<00:00, 42.81batch/s]\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Prototype Learning with Progress Bar\n",
    "print(\"Pretraining prototypes...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(train_loader, desc=\"Building Prototypes\", unit=\"batch\"):\n",
    "        images = images.to(DEVICE)\n",
    "        features = model.conv1(images)\n",
    "        features_flat = features.view(features.size(0), -1)\n",
    "        model.amm.add_prototypes(features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Supervised Training with Progress Bar\n",
    "print(\"Fine-tuning...\")\n",
    "model.train()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.float().to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Simplified Hebbian-inspired update\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply sparsity mask to gradients\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"conv1.weight\" in name:\n",
    "                    grad_mask = (param != 0).float()\n",
    "                    param.grad *= grad_mask\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def predict_cat(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prob = model(img_tensor).item()\n",
    "    return \"Cat\" if prob > 0.5 else \"Non-Cat\"\n",
    "\n",
    "# Example Prediction\n",
    "print(predict_cat(\"your_cat_image.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
